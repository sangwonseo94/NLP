{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"기상청은 슈퍼컴퓨터도 서울지역의 집중호우를 제대로 예측하지 못했다고 설명했습니다. 왜 오류가 발생했는지 자세히 분석해 예측 프로그램을 보완해야할 대목입니다. 관측 분야는 개선될 여지가 있습니다. 지금 보시는 왼쪽 사진이 현재 천리안 위성이 촬영한 것이고 오른쪽이 올해 말 쏘아 올릴 천리안 2A호가 촬영한 영상입니다. 오른쪽이 왼쪽보다 태풍의 눈이 좀 더 뚜렷하고 주변 구름도 더 잘 보이죠. 새 위성을 통해 태풍 구름 등의 움직임을 상세 분석하면 좀 더 정확히 예측을 할 수 있지 않을까 기대해 봅니다. 정구희 기자(koohee@sbs.co.kr)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re.sub(r'\\([a-zA-Z0-9\\._+]+@[a-zA-Z]+\\.(com|org|edu|net|co.kr)\\)', '', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('a+b*')\n",
    "r.findall('aaaa, cc, bbbb, aabbbb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('[A-Z]+')\n",
    "r.findall('HOME, home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('^a..')\n",
    "r.findall('abc,cba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile('a{2,3}b{2,3}')\n",
    "r.findall('aabb, aaabb, ab, aab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('.+:')\n",
    "m = p.search('http://google.com')\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('(내|나의|내꺼)')\n",
    "p.sub('그의', '나의 물건에 손대지 마시오.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 대소문자 통일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'Hello World'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 숫자, 문장부호, 특수문자 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('[0-9]+')\n",
    "p.sub(\"\", \"서울 부동산 가격이 올해 들어 평균 30% 상승했습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('\\W+')\n",
    "p.sub(\" \", \"★서울 부동산 가격이 올해 들어 평균 30% 상승했습니다!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('\\W+')\n",
    "s = p.sub(' ', '주제_1: 건강한 몸과 건강한 정신!')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = re.compile('_')\n",
    "p.sub(' ', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 불용어 처리\n",
    "words = ['소설', '시', '인문', '역사', '예술', '종교', '사회', '과학','경제 경영', '자기계발', '만화', '여행', '잡지',\n",
    "        '어린이', '청소년','유아', '요리','육아', '가정 살림', '건강 취미', '대학교재','국어와 외국어', 'IT 모바일',\n",
    "        '수험서 자격증', '초등참고서', '중고등참고서']   #yes24.com의 Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['유아', '청소년', '어린이', '육아', '성인', '19금']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in words if word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 불용어 처리\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['chief', 'justice', 'roberts', 'president', 'carter', 'president', 'clinton', \n",
    "         'president', 'bush', 'president', 'obama', 'fellow', 'americans', 'and', 'people', \n",
    "         'of', 'the', 'world', 'thank', 'you']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word for word in words if not word in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = \"\"\"It is important to be immersed while you are pythoning with python. All pythoners \n",
    "            have pythoned pooly at least once.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    print(ps.stem(word), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words :\n",
    "    print(ls.stem(word), end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words :\n",
    "    print(rs.stem(word), end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. N-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n번 연이어 등장하는 단어들의 연쇄를 의미\n",
    "- 두 번 연이어 등장하면 바이그램, 세 번 연이어 등장하면 트라이그램이라 한다.\n",
    "- 트라이그램 이상은 보편적으로 활용하지 않는다.\n",
    "- N그램은 보편적으로 영어에만 적용된다.\n",
    "- 예)'Republic of Korea', 'United Kongdom' 같은 경우 N그램을 활용해야 제대로 된 단어로 인지가능함.\n",
    "- 그렇지만 무작정 N그램을 적용하면 의미 없는 단어 뭉치가 많이 발생하여 불필요한 작업이 될 수 있다.\n",
    "- 따라서 유니그램(1-gram)과 혼합하여 단어들을 도출하는 것이 가장 이상적이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentense = \"\"\"Chief Justice Roberts, President Carter, President Clinton, President Bush, \n",
    "            President Obama, fellow Americans and people of the world, thank you. We, the citizens \n",
    "            of America are now joined in a great national effort to rebuild our country and restore \n",
    "            its promise for all of our people. Together, we will determine the course of America and \n",
    "            the world for many, many years to come. We will face challenges. We will confront hardships, \n",
    "            but we will get the job done.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = ngrams(sentense.split(), 2)   # 2개씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gram in grams:\n",
    "    print(gram, end = ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thekop/anaconda3/envs/py37/lib/python3.7/site-packages/jpype/_core.py:210: UserWarning: \n",
      "-------------------------------------------------------------------------------\n",
      "Deprecated: convertStrings was not specified when starting the JVM. The default\n",
      "behavior in JPype will be False starting in JPype 0.8. The recommended setting\n",
      "for new code is convertStrings=False.  The legacy value of True was assumed for\n",
      "this session. If you are a user of an application that reported this warning,\n",
      "please file a ticket with the developer.\n",
      "-------------------------------------------------------------------------------\n",
      "\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['친척들',\n",
       " '이',\n",
       " '모이',\n",
       " 'ㄴ',\n",
       " '이번',\n",
       " '추석',\n",
       " '차례상',\n",
       " '에서는',\n",
       " '단연',\n",
       " \"'\",\n",
       " '집값',\n",
       " \"'\",\n",
       " '이',\n",
       " '화제',\n",
       " '에',\n",
       " '오르',\n",
       " '아다',\n",
       " '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"친척들이 모인 이번 추석 차례상에서는 단연 '집값'이 화제에 올랐다.\"\n",
    "hannanum.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['친척들', '이번', '추석', '차례상', '집값', '화제']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('친척들', 'N'),\n",
       " ('이', 'J'),\n",
       " ('모이', 'P'),\n",
       " ('ㄴ', 'E'),\n",
       " ('이번', 'N'),\n",
       " ('추석', 'N'),\n",
       " ('차례상', 'N'),\n",
       " ('에서는', 'J'),\n",
       " ('단연', 'M'),\n",
       " (\"'\", 'S'),\n",
       " ('집값', 'N'),\n",
       " (\"'\", 'S'),\n",
       " ('이', 'J'),\n",
       " ('화제', 'N'),\n",
       " ('에', 'J'),\n",
       " ('오르', 'P'),\n",
       " ('아다', 'E'),\n",
       " ('.', 'S')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hannanum.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['친척',\n",
       " '들',\n",
       " '이',\n",
       " '모이',\n",
       " 'ㄴ',\n",
       " '이번',\n",
       " '추석',\n",
       " '차례',\n",
       " '상',\n",
       " '에서',\n",
       " '는',\n",
       " '단연',\n",
       " \"'\",\n",
       " '집',\n",
       " '값',\n",
       " \"'\",\n",
       " '이',\n",
       " '화제',\n",
       " '에',\n",
       " '오르',\n",
       " '았',\n",
       " '다',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['친척', '이번', '추석', '차례', '차례상', '상', '집', '집값', '값', '화제']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('친척', 'NNG'),\n",
       " ('들', 'XSN'),\n",
       " ('이', 'JKS'),\n",
       " ('모이', 'VV'),\n",
       " ('ㄴ', 'ETD'),\n",
       " ('이번', 'NNG'),\n",
       " ('추석', 'NNG'),\n",
       " ('차례', 'NNG'),\n",
       " ('상', 'NNG'),\n",
       " ('에서', 'JKM'),\n",
       " ('는', 'JX'),\n",
       " ('단연', 'MAG'),\n",
       " (\"'\", 'SS'),\n",
       " ('집', 'NNG'),\n",
       " ('값', 'NNG'),\n",
       " (\"'\", 'SS'),\n",
       " ('이', 'MDT'),\n",
       " ('화제', 'NNG'),\n",
       " ('에', 'JKM'),\n",
       " ('오르', 'VV'),\n",
       " ('았', 'EPT'),\n",
       " ('다', 'EFN'),\n",
       " ('.', 'SF')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kkma.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.phrases(text)   #Okt에만 있는 phrases() : 문장을 구 단위로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = \"The litte yellow dog barked at the Persian cat.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_en = pos_tag(tokens)\n",
    "tag_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "retokenizer = RegexpTokenizer('[\\w]+')\n",
    "emma = nltk.corpus.gutenberg.raw('austen-emma.txt')\n",
    "emma = retokenizer.tokenize(emma)\n",
    "emma = ' '.join(emma)\n",
    "emma = emma[44:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#품사에 대한 자세한 설명\n",
    "nltk.help.upenn_tagset('NNP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tagged_list = pos_tag(word_tokenize(emma[:100]))\n",
    "tagged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#문장에서 명사만 추출\n",
    "nouns_list = [tag[0] for tag in tagged_list if tag[1] == 'NN']\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규표현식을 사용해서 모든 명사(NN, NNPS, NNS)들 찾기\n",
    "import re\n",
    "nouns_list = [tag[0] for tag in tagged_list if re.compile(r'NN?').match(tag[1])]\n",
    "nouns_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#untag() 품사 제거한 list\n",
    "from nltk.tag import untag\n",
    "untag(tagged_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#단어와 품사를 붙여서 출력\n",
    "print([\"/\".join(tag) for tag in tagged_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining - 단어 빈도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/instructor/Downloads/트럼프취임연설문.txt'\n",
    "mydoc = None\n",
    "with open(path, 'r') as f:\n",
    "    mydoc = f.read()\n",
    "    \n",
    "mydoc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = mydoc.lower()\n",
    "tokens = tokenizer.tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopped_tokens = [word for word in list(tokens) if not word in stop_words]\n",
    "stopped_tokens2 = [word for word in stopped_tokens if len(word) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "myseries = pd.Series(stopped_tokens2).value_counts()\n",
    "myseries.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = myseries.index\n",
    "mydic = {}\n",
    "i = 1\n",
    "for key in index_list:\n",
    "    mydic[key] = i\n",
    "    i += 1\n",
    "    \n",
    "mydic   #Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/data/문재인대통령취임연설문.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bd1c6ebe308b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/data/문재인대통령취임연설문.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmydoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euckr'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmydoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/data/문재인대통령취임연설문.txt'"
     ]
    }
   ],
   "source": [
    "path = '/home/data/문재인대통령취임연설문.txt'\n",
    "mydoc = None\n",
    "with open(path, 'r', encoding='euckr') as f:\n",
    "    mydoc = f.read()\n",
    "    \n",
    "mydoc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum\n",
    "hannanum = Hannanum()\n",
    "mylist = [word for word in hannanum.nouns(mydoc) if len(word) > 1]   #한글자 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([mydoc])\n",
    "tokenizer.word_index   #Integer Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
