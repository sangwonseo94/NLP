이전 토큰화는 단위를 만드는 즉 분류하는 작업이였다면, 이전,이후에는 텍스트 데이터를 용도에 맞게
 cleaning/ normalization을 해야한다. 필수이다.

cleaning의 가장쉬운예는 구두점을 분류시키는것이다. 물론 구두점을 막 없애기는 힘들지만 개념만 잡고간다.
즉 노이즈를 제거하는 작업이지만 완벽할 수 없다.
-> 불필요한 단어를 제거한다 Removing Unnecessary Words : 불용어를 제거하거나 등장빈도가 너무적은단어, 길이가 짧은 단어를 제거
등장 빈도 적은단어 제거 (removing rare words) : 100000개에 5개정도 나오는 단어는 제거하자
하지만 길이가 짧은 단어는 막삭제하면 안된다 영어는 평균길이가 6-7인데
한국어는 2-3정도이므로 짧은 길이여도 많은 내용을 가지고 있을 수 있다.

정규화 => 의미가 비슷하거나 똑같은 단어를 함께 찾을 수 있게 하는 방법
단어를 통합한다? =? 어간을 추출하는 방법 stemming 표제어를 추출 lemmatiozation

정규화에는 여러타입이 있다 
1. 대소문자 통합


즉 이러한 처리를 위해서는 정규 표현식이 필수이다 .(Regular Expression)
import re 을 통하여 처리를 해보자 