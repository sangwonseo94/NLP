과거의 규칙을 기계에게 학습시킨다.

var.item() => content를 뽑아냄

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

requires_gard= True // 하이퍼 파라미터들을 학습하겠씁니다.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

optimizer.zero_grad()
cost.backward()
optimizer.step() 항상 붙어다니는 optimizer 선언후에..

control + / = block으로 주석처리 
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

당연하게도 “good learning rate”를 선택하는게 좋은, 어떻게 보면 약간 바보 같은 질문일 수 있겠다. 하지만 가장 좋은 learning rate는 초기에는 높은 learning rate로 학습 시키다 점점 learning rate를 감소시키는 것이 가장 효과적일 것이다.

때문에 SGD 베이스 알고리즘을 사용하는 깊은 네트워크를 학습시킬 때 learning rate를 decay 하는 것이 도움이 될 수 있다. 이 때 learning rate를 얼마나 decay 하냐에 따라 성능이 좌우된다. 만약 decay를 천천히 한다면 파라미터가 요동치면서 수렴하기 때문에 매우 느리게 수렴하며, decay를 빠르게 하면 시스템이 너무 일찍 식기 때문에 최적점에 도달하기 전에 멈춰버릴 수도 있다. 다음은 learning rate의 decay를 구현하는 일반적인 방법 세가지를 소개한다.

Step decay는 몇 번의 epoch마다 일정하게 learning rate를 줄이는 방법이다. 보통 5번의 epoch마다 절반으로 줄이거나, 매 20번의 epoch마다 0.1배로 줄인다. 얼마나, 언제 줄일지는 풀어야 할 문제와 모델에 따라 다른데, 실제로 이 수치를 휴리스틱하게 정하는 방법은 트레이닝 시 validation 에러를 관찰하는 것이다. 1/t decay는 a=a01+kt 에 의해 learning rate를 결정한다. Exponential decay : a=a0e?kt에 의해 learning rate를 결정한다. 이 때 1/t decay와 exponential decay에서 a0, k는 hyperparameter이며, t는 반복 횟수이다.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
def init_model(input_size, hidden_size, output_size):
    # 모델 초기화
    model = {}
    model["W1"] = 0.0001 * np.random.rand(input_size, hidden_size)
    model["b1"] = np.zeros(hidden_size)
    model["W2"] = 0.0001 * np.random.rand(hidden_size, output_size)
    model["b2"] = np.zeros(hidden_size)
    
    return model

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
   print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(
            epoch, nb_epochs, W, b, cost.item()

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
기본적으로 PyTorch는 NCHW 형태이다.

N : BATCH SIZE
C : CHANNEL 
H : HEIGHT
W : WIDTH
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

cost = mse를 w로 편미분하여서 알파를 곱해서 w를 업데이트한다.

그래프를 그릴때에는 x값과 해당되는 y값을 모아서 리스트로 만들어놔야한다.
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

time series data는 data shuffle을 실행하면 안된다.

